#ML #анализ_данных 
# Модель линейной регрессии

w - вектор весов, x - вектор признаков

$$
a(x) = \mathbf{w} \cdot \mathbf{x}
$$
$$
a(x) = w_0 + \sum_{j=1}^{n} w_j x_{ij}
$$
В первой формуле свободный коэффициент w_0 занесен в вектор w, а в вектор x занесен фиктивный признак, всегда равный 1.

# Области применения линейной регрессии

Линейная регрессия предполагает, что каждый признак линейно влияет на целевую переменную. Также никак не учитывается, что целевая переменная может как-то зависеть от некоторых комбинаций признаков. Это вряд ли выполнено по умолчанию, поэтому линейные модели требуют определенной [[ML_Предобработка данных]] Это подразумевает в себе специфичную обработку некоторых видов [[ML_признаки | признаков]]

## Категориальные признаки

Это признак, который имеет ограниченное количество не числовых значений. Это не числовой признак, но нам нужно преобразовать его в числовой, чтобы удобно с ним работать. 

### One-Hot кодирование

One-hot кодирование — это метод преобразования категориальных признаков в бинарные.

Пусть у нас есть категориальный признак f(x), который принимает значения из множества C. Мы преобразуем его в m бинарных признаков следующим образом:

$$ b_i(x) = \begin{cases}  1 & \text{если } f(x) = c_i \\ 0 & \text{иначе} \end{cases} $$

Так как для каждого объекта выполнено:

$$\sum_{i=1}^{m} b_i(x) = 1 $$
признаки являются линейно зависимыми. Чтобы избежать этой зависимости, один из бинарных признаков может быть исключен.


## Работа с текстами

Если признак представлен произвольным текстом, то к нему тоже нужен особый подход.

### Bag Of Words

Метод "bag of words" (мешок слов) — это подход к представлению текстовых данных, при котором текст кодируется в виде вектора, где каждый элемент соответствует определенному слову из словаря.

Пусть у нас есть словарь, состоящий из слов c1​,...,cm​. Каждый текст x кодируется вектором из m признаков {b1(x), b2(x)...b_m(x)}, где каждый признак b_j​(x) равен количеству вхождений слова c_j​ в текст.

$$ b_j(x) = \text{количество вхождений слова } c_j \text{ в текст } x $$

Линейная модель на основе таких признаков имеет следующий вид:

$$ a(x) = w_1b_1(x) + \dots + w_mb_m(x) $$
Здесь каждое вхождение слова c_j​ в текст меняет прогноз на w_j​. Например, наличие слова "престижный" может указывать на высокую стоимость квартиры, в то время как слово "плохой" вряд ли будет использоваться в описании дорогой квартиры.

## Бинаризация числовых признаков

Бинаризация числовых признаков — это метод преобразования непрерывных признаков в бинарные, путем разделения их на интервалы.

Рассмотрим пример с расстоянием до ближайшей станции метро x_j​. Если зависимость цены квартиры от расстояния не является линейной, мы можем преобразовать этот признак, разбив его на интервалы.

Выберем сетку точек t1​,...,t_m​, которая может быть равномерной или основанной на эмпирических квантилях. Добавим к этой сетке точки t_0​=−∞ и t_m+1​=+∞.

Определим новые признаки следующим образом:


$$ b_i(x) = \begin{cases}  1 & \text{если } t_{i-1} < x_j \leq t_i \\ 0 & \text{иначе} \end{cases} $$
Линейная модель на основе этих признаков будет иметь вид:

$$ a(x) = w_1 \cdot [t_0 < x_j \leq t_1] + \dots + w_{m+1} \cdot [t_m < x_j \leq t_{m+1}] $$

Такой подход позволяет учесть нелинейную зависимость между признаком и целевой переменной, предоставляя отдельный прогноз для каждого интервала.


# Измерение ошибок в задачах регрессии

a - прогноз модели , y - целевая переменная

## MSE и R^2

$$
отклонение = L(y, a) = (a - y) ^ 2
$$
$$
функционал = MSE(a, X) = \frac{1}{l} \sum_{i=1}^{l} (a(x_i) - y_i)^2
$$
$$
RMSE = \sqrt{\frac{1}{l} \sum_{i=1}^{l} (a(x_i) - y_i)^2}
$$
Корень из ошибки нужен для большей репрезентативности данных.

MSE позволяет хорошо контролировать ошибку в процессе обучения так как является дифференцируемой везде функцией, но плохо показывает, насколько хорошо обучена модель. Для этого коэффициент детерминации - нормированная среднеквадратичная ошибка

$$
R^2(a, X) = 1 - \frac{\sum_{i = 1}^{l}{(a(x_i) - y_i)^2}}{\sum_{i = 1}^{l}{(y_i - \overline{y})^2}}
$$
Ее значения лежат от 0 до 1. Чем ближе к 1, тем лучше, чем ближе к 0 тем хуже.

## MAE

$$
отклонение = L(y, a) = |a - y|
$$

$$
функционал = MSE(a, X) = \frac{1}{l} \sum_{i=1}^{l} |a(x_i) - y_i|
$$
Модуль отклонения не является дифференцируемым, но при этом менее чувствителен к выбросам. Квадрат отклонения, по сути, делает особый акцент на объектах с сильной ошибкой, и метод обучения будет в первую очередь стараться уменьшить отклонения на таких объектах. Если же эти объекты являются выбросами (то есть значение целевой переменной на них либо ошибочно, либо относится к друго- му распределению и должно быть проигнорировано), то такая расстановка акцентов приведёт к плохому качеству модели. Модуль отклонения в этом смысле гораздо более терпим к сильным ошибкам.

Достойно написания и прочтения - [[ML_Сравнение функций ошибки]]

## MSLE
## MAPE и SMAPE
## Huber Loss
## Log Cosh

**Чтобы идти дальше, рекомендуется прочитать [[ML_Переобучение]]**

# Обучение линейной регрессии

Удивительно, но для линейной регрессии есть четкая формула для решения, но только если мы используем квадратичную функцию ошибки

$$
w = (X \overline{X})^{-1} * \overline{X} * Y
$$
Безусловно, наличие явной формулы для оптимального вектора весов — это большое преимущество линейной регрессии с квадратичным функционалом. Но данная формула не всегда применима по ряду причин: 

-  Обращение матрицы — сложная операция с кубической сложностью от количества признаков. Если в выборке тысячи признаков, то вычисления могут стать слишком трудоёмкими. Решить эту проблему можно путём использования численных методов оптимизации. 

- Матрица X^T * X может быть вырожденной или плохо обусловленной. В этом случае обращение либо невозможно, либо может привести к неустойчивым результатам.

Стоит заметить, что аналитическая формула - это большая редкость в ML, для обучения многих функций стоит иметь единый функционал, а не пытаться выводить для каждого свои формулы. И такой функционал есть - [[ML_3_Градиентный спуск]]
